# Internal Crisis Response Protocol (MVP)

**Last Updated:** January 2025  
**Owner:** Withinly Operations  
**Review Frequency:** Quarterly or after any crisis incident

---

## üéØ Purpose

This document outlines what YOU (the operator) do when a crisis situation is detected in user conversations.

---

## üö® What Constitutes a Crisis

### Immediate Action Required:

- Explicit suicidal intent ("I'm going to kill myself tonight")
- Active self-harm in progress
- Immediate danger to others ("I'm going to hurt my partner")
- Ongoing abuse with immediate safety risk

### Monitor But Don't Escalate:

- Past suicidal thoughts (not current intent)
- General depression/anxiety
- Relationship stress causing emotional pain
- Historical trauma discussion

---

## üìã MVP Response Process

### Step 1: AI Handles First Response

- The consultant automatically detects crisis language
- Provides crisis resources immediately
- Does NOT continue regular conversation
- This happens automatically‚Äîno manual intervention needed

### Step 2: Optional Manual Review (Beta Only)

**During Beta Testing:**

- Check conversation logs weekly for any crisis-related exchanges
- Look for: conversations that mention crisis resources or safety concerns
- No real-time monitoring required for MVP

**If You Find a Crisis Conversation:**

1. **Document It** (simple spreadsheet):
   - Date/Time
   - User ID (anonymized in notes)
   - Crisis type (suicidal/self-harm/abuse/danger to others)
   - AI response given
   - User's follow-up (if any)

2. **NO Direct Outreach** (privacy + liability concerns):
   - You are NOT a crisis service
   - Do not contact the user directly about crisis content
   - Do not try to "check in" on them
   - Exception: Only if legally required (see Step 3)

3. **Legal Obligation Check**:
   - Consult with lawyer BEFORE beta about mandatory reporting laws in your jurisdiction
   - In most places, you have NO duty to report (you're not a healthcare provider)
   - Exception: Child abuse disclosure may trigger reporting in some jurisdictions

### Step 3: Improve the System

After any crisis incident, review:

- Did the AI detect it appropriately?
- Were the resources helpful?
- Should the crisis prompt be adjusted?

---

## üîí Data Handling

### Crisis Conversation Storage:

- Treat crisis conversations with extra security
- Never share specifics in team discussions
- Use anonymized summaries only
- Delete after 90 days unless legally required to retain

### If Contacted by Emergency Services:

- You may receive requests for user data from law enforcement
- **Do not comply without legal consultation**
- Your privacy policy should state what you will/won't share
- Document all requests and your responses

---

## üìû Who to Contact

### Internal Escalation:

- **Beta Phase:** Just you, review weekly
- **Post-Launch:** Designate a "safety lead" if team grows

### External Support:

- **Legal Questions:** [Your lawyer's contact]
- **Technical Issues:** [Your developer contact]
- **Mental Health Consultation:** Consider retaining a licensed professional as advisor (optional, post-MVP)

---

## üõ°Ô∏è Liability Protection

### What Protects You:

1. **Clear disclaimers** in Terms of Service ("not therapy, not crisis support")
2. **Immediate redirection** to crisis resources (AI does this automatically)
3. **No false promises** (don't claim to prevent suicide or treat crisis)
4. **Professional resources** provided prominently
5. **Privacy respected** (no unauthorized wellness checks)

### What Increases Risk:

- Trying to "counsel" users through crisis
- Claiming therapeutic capabilities
- Ignoring clear crisis signals
- Making users feel you're their safety net

---

## üìä Beta Testing Goals

Track these during beta:

- How many crisis detections occurred?
- Did AI response work correctly?
- Did any users continue chatting after crisis response?
- What feedback did beta users give about crisis handling?

---

## üîÑ Post-Beta Improvements (Not MVP)

Consider later:

- Flagging system for human review
- Partnership with crisis text line for warm handoffs
- Training data to improve crisis detection
- User feedback mechanism ("Was this helpful?")

---

## ‚ö†Ô∏è Remember:

**You are NOT responsible for preventing suicide or stopping abuse.**  
**You ARE responsible for:**

- Having clear disclaimers
- Redirecting to appropriate resources
- Not making the situation worse
- Respecting privacy

**When in doubt:** Default to privacy + directing to professionals.

---

## Appendix: Sample Crisis Log

| Date       | User ID     | Crisis Type         | AI Response           | User Follow-up     | Notes                        |
| ---------- | ----------- | ------------------- | --------------------- | ------------------ | ---------------------------- |
| 2025-01-15 | user_abc123 | Suicidal ideation   | Resources provided    | Exited chat        | No further action needed     |
| 2025-01-20 | user_xyz789 | Relationship stress | Gentle resources note | Continued chatting | Not acute crisis, monitoring |

---

**Next Steps:**

1. Consult lawyer about mandatory reporting obligations BEFORE beta
2. Add this protocol to your internal documentation
3. Review after first 30 days of beta
4. Update as needed based on actual incidents
